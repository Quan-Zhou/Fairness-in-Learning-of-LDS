{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ncpol2sdpa import *\n",
    "from random import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, utils\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "# Odds equalizing post-processing algorithm\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.eq_odds_postprocessing import EqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mappings = {\n",
    "    #'label_maps': [{1.0: 'Did recid.', 0.0: 'No recid.'}],\n",
    "    'protected_attribute_maps': [{0.0: 'Male', 1.0: 'Female'},\n",
    "                                 {1.0: 'Caucasian', 0.0: 'Not Caucasian'}]\n",
    "}\n",
    "\n",
    "class Compas(StandardDataset):\n",
    "\n",
    "    def __init__(self, df,label_name='is_recid', favorable_classes=[0],\n",
    "                 protected_attribute_names=['sex', 'race'],\n",
    "                 privileged_classes=[['Female'], ['Caucasian']],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=['age_cat', 'c_charge_degree'],\n",
    "                 features_to_keep=['sex', 'age', 'age_cat', 'race',\n",
    "                     'juv_fel_count', 'juv_misd_count', 'juv_other_count',\n",
    "                     'priors_count', 'c_charge_degree'],\n",
    "                 features_to_drop=[], na_values=[],\n",
    "                 custom_preprocessing=[],\n",
    "                 metadata=default_mappings):\n",
    "        \"\"\"See :obj:`StandardDataset` for a description of the arguments.\n",
    "        Note: The label value 0 in this case is considered favorable (no\n",
    "        recidivism).\n",
    "        Examples:\n",
    "            In some cases, it may be useful to keep track of a mapping from\n",
    "            `float -> str` for protected attributes and/or labels. If our use\n",
    "            case differs from the default, we can modify the mapping stored in\n",
    "            `metadata`:\n",
    "            >>> label_map = {1.0: 'Did recid.', 0.0: 'No recid.'}\n",
    "            >>> protected_attribute_maps = [{1.0: 'Male', 0.0: 'Female'}]\n",
    "            >>> cd = CompasDataset(protected_attribute_names=['sex'],\n",
    "            ... privileged_classes=[['Male']], metadata={'label_map': label_map,\n",
    "            ... 'protected_attribute_maps': protected_attribute_maps})\n",
    "            Now this information will stay attached to the dataset and can be\n",
    "            used for more descriptive visualizations.\n",
    "        \"\"\"\n",
    "        super(Compas, self).__init__(df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)\n",
    "\n",
    "def split_index(index):\n",
    "    length = len(index)\n",
    "    # define the ratios 8:2\n",
    "    train_len = int(length * 0.8)\n",
    "\n",
    "    # split the dataframe\n",
    "    #idx = [*range(length)]\n",
    "    shuffle(index)  # shuffle the index\n",
    "    I_train = index[:train_len]\n",
    "    I_test = index[train_len:length]\n",
    "\n",
    "    return [I_train,I_test]\n",
    "\n",
    "def aif_postprocess(dataframe,Itrain,Itest,method):\n",
    "    Strain,Strain_pred=compas_pred(Itrain)\n",
    "    Stest,Stest_pred=compas_pred(Itest)\n",
    "    randseed = 12345679\n",
    "    if method=='RejectOption':\n",
    "        cpp=RejectOptionClassification(privileged_groups = privileged_groups,unprivileged_groups = unprivileged_groups)\n",
    "    elif method=='EqOddsPostprocessing':\n",
    "        cpp=EqOddsPostprocessing(privileged_groups = privileged_groups,unprivileged_groups = unprivileged_groups)\n",
    "    else:\n",
    "    #cost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n",
    "    #else:\n",
    "        cpp=CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,unprivileged_groups = unprivileged_groups,\n",
    "                                             cost_constraint=method)  #,seed=randseed\n",
    "    cpp = cpp.fit(Strain,Strain_pred)\n",
    "    return cpp.predict(Stest_pred).scores #labels #[Stest.labels,\n",
    "\n",
    "def compas_pred(index):\n",
    "    Sindex=Compas(df=dataframe.loc[index])\n",
    "    Sindex_pred=Sindex.copy(deepcopy=True)\n",
    "    Sindex_pred.scores=np.array([[1-i/10] for i in dataframe.loc[index,'decile_score'].tolist()])\n",
    "    #Sindex_pred.labels = np.where(Sindex_pred.scores >= thresh,Sindex_pred.favorable_label,Sindex_pred.unfavorable_label)\n",
    "    #y_pred = np.zeros_like(Sindex.labels)\n",
    "    #y_pred[dataframe.loc[index,'decile_score']>thresh]=Sindex.unfavorable_label\n",
    "    #Sindex_pred.labels=y_pred\n",
    "    return [Sindex,Sindex_pred]\n",
    "\n",
    "def preprocess(filepath,nrows,column_names):\n",
    "    dataframe = pd.read_csv(filepath, index_col='id',nrows=nrows)\n",
    "    dataframe=dataframe[(dataframe[\"race\"]=='African-American')|(dataframe[\"race\"]=='Caucasian')]\n",
    "    dataframe[(dataframe.days_b_screening_arrest <= 30)\n",
    "                & (dataframe.days_b_screening_arrest >= -30)\n",
    "                & (dataframe.is_recid != -1)\n",
    "                & (dataframe.c_charge_degree != 'O')\n",
    "                & (dataframe.score_text != 'N/A')]\n",
    "    dataframe=dataframe[column_names] #[features+labels]\n",
    "    dataframe=dataframe.dropna(axis=0, how='any',thresh=None,subset=None,inplace=False)\n",
    "    \n",
    "    dataframe['priors_total_count']=(dataframe['juv_fel_count']+dataframe['juv_misd_count']+dataframe['priors_count'])/3 #+dataframe['juv_other_count']\n",
    "    \n",
    "    # base rate: 1-P(is_recid=1|S=s)\n",
    "    base0=(1-dataframe[(dataframe[\"race\"]=='Caucasian') & (dataframe[\"is_recid\"]==1)].shape[0]/dataframe[dataframe[\"race\"]=='Caucasian'].shape[0])*100\n",
    "    base1=(1-dataframe[(dataframe[\"race\"]!='Caucasian') & (dataframe[\"is_recid\"]==1)].shape[0]/ dataframe[dataframe[\"race\"]!='Caucasian'].shape[0])*100\n",
    "    return dataframe,[base0,base1]\n",
    "\n",
    "def training_process(dataframe,Itrain,method): #Itrain,Itest,method\n",
    "    Dtrain=dataframe.loc[Itrain]\n",
    "    I0_train=Dtrain[Dtrain['race']=='Caucasian'].index\n",
    "    I1_train=Dtrain[Dtrain['race']!='Caucasian'].index\n",
    "    level = 1\n",
    "    \n",
    "    if method=='subgroup-fair':\n",
    "        # Decision Variables\n",
    "        a = generate_operators(\"a\", n_vars=2, hermitian=True, commutative=False) # age < 25\n",
    "        c = generate_operators(\"c\", n_vars=2, hermitian=True, commutative=False) # total number of previous convictions\n",
    "        d = generate_operators(\"d\", n_vars=2, hermitian=True, commutative=False)\n",
    "        e = generate_operators(\"e\", n_vars=2, hermitian=True, commutative=False)\n",
    "        z = generate_operators(\"z\", n_vars=3, hermitian=True, commutative=True)\n",
    "\n",
    "        # Constraints\n",
    "        ine1 = [z[0]+Dtrain.loc[i,\"is_recid\"] - a[0]*int(Dtrain.loc[i,'age']<25) - c[0]*Dtrain.loc[i,'priors_total_count'] - d[0]*Dtrain.loc[i,'decile_score'] + e[0] for i in I0_train]\n",
    "        ine2 = [z[0]-Dtrain.loc[i,\"is_recid\"] + a[0]*int(Dtrain.loc[i,'age']<25) + c[0]*Dtrain.loc[i,'priors_total_count'] + d[0]*Dtrain.loc[i,'decile_score'] + e[0] for i in I0_train]\n",
    "        ine3 = [z[0]+Dtrain.loc[i,\"is_recid\"] - a[1]*int(Dtrain.loc[i,'age']<25) - c[1]*Dtrain.loc[i,'priors_total_count'] - d[1]*Dtrain.loc[i,'decile_score'] + e[1] for i in I1_train]\n",
    "        ine4 = [z[0]-Dtrain.loc[i,\"is_recid\"] + a[1]*int(Dtrain.loc[i,'age']<25) + c[1]*Dtrain.loc[i,'priors_total_count'] + d[1]*Dtrain.loc[i,'decile_score'] + e[1] for i in I1_train]\n",
    "        max1 =[z[1]-sum((Dtrain.loc[i,\"is_recid\"]-a[0]*int(Dtrain.loc[i,'age']<25) - c[0]*Dtrain.loc[i,'priors_total_count'] - d[0]*Dtrain.loc[i,'decile_score'] + e[0])**2 for i in I0_train)/len(I0_train)]\n",
    "        max2 =[z[2]-sum((Dtrain.loc[i,\"is_recid\"]-a[1]*int(Dtrain.loc[i,'age']<25) - c[1]*Dtrain.loc[i,'priors_total_count'] - d[1]*Dtrain.loc[i,'decile_score'] + e[1])**2 for i in I1_train)/len(I1_train)]\n",
    "        \n",
    "        obj_D = z[0] + z[1] + z[2] + 0.5*(z[2]-z[1]) #+ 0.01*(e[0]**2 + e[1]**2)\n",
    " \n",
    "    elif method=='instant-fair':\n",
    "\n",
    "        # Decision Variables\n",
    "        a = generate_operators(\"a\", n_vars=2, hermitian=True, commutative=False) # age < 25\n",
    "        c = generate_operators(\"c\", n_vars=2, hermitian=True, commutative=False) # total number of previous convictions\n",
    "        d = generate_operators(\"d\", n_vars=2, hermitian=True, commutative=False)\n",
    "        e = generate_operators(\"e\", n_vars=2, hermitian=True, commutative=False)\n",
    "        z = generate_operators(\"z\", n_vars=2, hermitian=True, commutative=True)\n",
    "\n",
    "        # Constraints\n",
    "        ine1 = [(z[0]+Dtrain.loc[i,\"is_recid\"] - a[0]*int(Dtrain.loc[i,'age']<25) - c[0]*Dtrain.loc[i,'priors_total_count'] - d[0]*Dtrain.loc[i,'decile_score'] + e[0])/len(I0_train) for i in I0_train]\n",
    "        ine2 = [(z[0]-Dtrain.loc[i,\"is_recid\"] + a[0]*int(Dtrain.loc[i,'age']<25) + c[0]*Dtrain.loc[i,'priors_total_count'] + d[0]*Dtrain.loc[i,'decile_score'] + e[0])/len(I0_train) for i in I0_train]\n",
    "        ine3 = [(z[0]+Dtrain.loc[i,\"is_recid\"] - a[1]*int(Dtrain.loc[i,'age']<25) - c[1]*Dtrain.loc[i,'priors_total_count'] - d[1]*Dtrain.loc[i,'decile_score'] + e[1])/len(I1_train) for i in I1_train]\n",
    "        ine4 = [(z[0]-Dtrain.loc[i,\"is_recid\"] + a[1]*int(Dtrain.loc[i,'age']<25) + c[1]*Dtrain.loc[i,'priors_total_count'] + d[1]*Dtrain.loc[i,'decile_score'] + e[1])/len(I1_train) for i in I1_train]\n",
    "        max1 = [(z[1]+(Dtrain.loc[i,\"is_recid\"]-a[0]*int(Dtrain.loc[i,'age']<25) - c[0]*Dtrain.loc[i,'priors_total_count'] - d[0]*Dtrain.loc[i,'decile_score'] + e[0]))/len(I0_train) for i in I0_train]\n",
    "        max2 = [(z[1]-(Dtrain.loc[i,\"is_recid\"]-a[1]*int(Dtrain.loc[i,'age']<25) - c[1]*Dtrain.loc[i,'priors_total_count'] - d[1]*Dtrain.loc[i,'decile_score'] + e[1]))/len(I1_train) for i in I1_train]\n",
    "       \n",
    "        obj_D = z[0] + z[1] #+ 0.01*(e[0]**2 + e[1]**2)\n",
    "    \n",
    "    ine_D=ine1+ine2+ine3+ine4+max1+max2\n",
    "    sdp_D = SdpRelaxation(variables = flatten([a,c,d,e,z]), verbose = 2)\n",
    "    sdp_D.get_relaxation(level, objective=obj_D, inequalities=ine_D)\n",
    "    #sdp_D.solve(solver='mosek')\n",
    "    sdp_D.solve(solver='sdpa', solverparameters={\"executable\":\"sdpa_gmp\",\"executable\": \"C:\\\\Users\\\\zhouq\\\\Documents\\\\sdpa7-windows\\\\sdpa.exe\"})\n",
    "    print(sdp_D.primal, sdp_D.dual, sdp_D.status)\n",
    "    \n",
    "    return [sdp_D[a[0]],sdp_D[a[1]],sdp_D[c[0]],sdp_D[c[1]],sdp_D[d[0]],sdp_D[d[1]],sdp_D[e[0]],sdp_D[e[1]]]\n",
    "\n",
    "def normalize_score1(arr):\n",
    "    # Min-Max normalized scores after deleting outliers.\n",
    "     #Outliers are set to 0 (if too small) or 1 (if too large) directly.\n",
    "    outlierPosition=detect_outliers(arr,3)\n",
    "    arr_clean = np.delete(arr,outlierPosition)\n",
    "    #arr_clean=arr\n",
    "    arr_min=np.min(arr_clean)\n",
    "    arr_max=np.max(arr_clean)\n",
    "\n",
    "    normalized_arr = np.array([round(float(x - arr_min)/(arr_max - arr_min),1) for x in arr])\n",
    "    #arr_nor = [int(10*float(x - np.mean(arr)/np.std(arr)) ) for x in arr]\n",
    "    normalized_arr[normalized_arr>10]=1\n",
    "    normalized_arr[normalized_arr<0]=0\n",
    "    return normalized_arr\n",
    "\n",
    "def detect_outliers(data,threshold):\n",
    "    # return the location of outliers.\n",
    "    mean_d = np.mean(data)\n",
    "    std_d = np.std(data)\n",
    "    outliers = []\n",
    "    for i in range(len(data)):\n",
    "        z_score= (data[i] - mean_d)/std_d \n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    return outliers\n",
    "    \n",
    "def new_postprocess(dataframe,Itrain,Itest,method):\n",
    "    a0,a1,c0,c1,d0,d1,e0,e1=training_process(dataframe,Itrain,method)  \n",
    "    arr=[]\n",
    "    for i in Itest:\n",
    "        if dataframe.loc[i,'race']=='Caucasian':\n",
    "            arr+=[a0*int(dataframe.loc[i,'age']<25) + c0*dataframe.loc[i,'priors_total_count'] + d0*dataframe.loc[i,'decile_score'] + e0]\n",
    "        elif dataframe.loc[i,'race']!='Caucasian':\n",
    "            arr+=[a1*int(dataframe.loc[i,'age']<25) + c1*dataframe.loc[i,'priors_total_count'] + d1*dataframe.loc[i,'decile_score'] + e1]\n",
    "    \n",
    "    normalized_arr=normalize_score1(np.array(arr))\n",
    "    return normalized_arr \n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "# Output: Positive rate, False positive rate; False negative rate; True positive rate\n",
    "# Positive event is being predicted not to re-offend  0\n",
    "# Negative event is being predicted to re-offend  1\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0 # False Positive\n",
    "    FN = 0 # False Negative\n",
    "    \n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "            TP += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "            TN += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    "    \n",
    "    #PR =(TP+FP)/len(y_hat) # Positive rate\n",
    "    NR =(TN+FN)/len(y_hat) # Negative rate\n",
    "    FPR=FP/(FP+TN+10**(-6)) # False positive rate # + 0.001 to avoid being divided by zero # 干了坏事没被发现\n",
    "    #TPR=TP/(TP+FN+10**(-6)) # True positive rate # recall/sensitivity # 没干坏事没被冤枉\n",
    "    FNR=FN/(FN+TP+10**(-6)) # False Omission Rate # 没干坏事却被冤枉\n",
    "    PPV=TP/(TP+FP+10**(-6)) # positive predictive value (PPV) precision #  成功预测出好人\n",
    "    NPV=TN/(TN+FN+10**(-6)) # negative predictive value (NPV) # 成功预测出坏人\n",
    "    inACC=FP+FN # False prediction number\n",
    "    \n",
    "    return [NR,FPR,FNR,PPV,NPV,inACC]\n",
    "\n",
    "def fair_metric(Dtest,I0_test,I1_test,score_name,thresh,base_rate):\n",
    "    # base_rate override thresh\n",
    "    if len(base_rate)!=0:\n",
    "        th0=np.percentile(Dtest[score_name],[base_rate[0]])[0]\n",
    "        th1=np.percentile(Dtest[score_name],[base_rate[1]])[0]\n",
    "    else:\n",
    "        th0=np.percentile(Dtest[score_name],[thresh])[0]\n",
    "        th1=th0\n",
    "  \n",
    "    #print('is I0_test = I1_test: ',len(I0_test)==len(I1_test))\n",
    "    y_actual_I0=Dtest.loc[I0_test,\"is_recid\"].tolist()\n",
    "    y_hat_I0=(Dtest.loc[I0_test,score_name]>=th0).astype(int).tolist()\n",
    "    #y_compas_I0=(compas_test.loc[I0_test,'compas_decile_score']>=th).astype(int).tolist()\n",
    "\n",
    "    y_actual_I1=Dtest.loc[I1_test,\"is_recid\"].tolist()\n",
    "    y_hat_I1=(Dtest.loc[I1_test,score_name]>=th1).astype(int).tolist()\n",
    "    #y_compas_I1=(compas_test.loc[I1_test,'compas_decile_score']>=th).astype(int).tolist()\n",
    "    \n",
    "    perf_I0=perf_measure(y_actual_I0, y_hat_I0)\n",
    "    perf_I1=perf_measure(y_actual_I1, y_hat_I1)\n",
    "\n",
    "    IND=abs(perf_I0[0]-perf_I1[0])\n",
    "    SP =abs(perf_I0[1]-perf_I1[1]+abs(perf_I1[2]-perf_I0[2]))  # abs(perf_I0[1]-perf_I1[1])+\n",
    "    #print((perf_I0[2]-perf_I1[2]))\n",
    "    SF =abs(perf_I0[3]-perf_I1[3]+abs(perf_I0[4]-perf_I1[4])) #abs(perf_I0[3]-perf_I1[3])+\n",
    "    #print((perf_I0[4]-perf_I1[4]))\n",
    "    INA=(perf_I0[5]+perf_I1[5])/(len(I0_test)+len(I1_test)) #Dtest.shape[0] # perdiction performance -- inaccuracy\n",
    "    return [IND,SP,SF,INA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "level=1\n",
    "filepath = 'compas-scores-two-years.csv'\n",
    "nrows=1200\n",
    "\n",
    "features=['sex', 'age', 'age_cat', 'race', 'juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'c_charge_degree']\n",
    "labels=[\"is_recid\",'decile_score']\n",
    "performace=['IND','SP','SF','INA','INDrw','SPrw','SFrw','INArw']\n",
    "\n",
    "unprivileged_groups = [{'race': 1}]  \n",
    "privileged_groups = [{'race': 0}]\n",
    "\n",
    "dataframe,base_rate=preprocess(filepath,nrows,features+labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59.05707196029777, 46.179401993355484]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataframe.race=='Caucasian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataframe.race!='Caucasian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## =================\n",
    "trials=50\n",
    "\n",
    "methods=['COMPAS',\"weighted\"] #,\"subgroup-fair\",\"instant-fair\"\n",
    "\n",
    "metric=pd.DataFrame(columns=performace+['type','thresh','trial'])\n",
    "for ignore in range(trials):\n",
    "    Itrain,Itest=split_index(dataframe.index.tolist())\n",
    "    Dtest=dataframe.loc[Itest]\n",
    "    I0_test=Dtest[Dtest['race']=='Caucasian'].index\n",
    "    I1_test=Dtest[Dtest['race']!='Caucasian'].index\n",
    "    \n",
    "    # update scores:\n",
    "    Dtest['COMPAS']=Dtest['decile_score']/10\n",
    "    for m in methods[1:]:\n",
    "        if m not in ['subgroup-fair',\"instant-fair\"]:\n",
    "            pp=aif_postprocess(dataframe,Itrain,Itest,m).flatten()\n",
    "        else:\n",
    "            pp=new_postprocess(dataframe,Itrain,Itest,m)\n",
    "        i=0\n",
    "        for j in Itest:\n",
    "            Dtest.loc[j,m]=pp[i]\n",
    "            i+=1\n",
    "\n",
    "    len_rw=min(len(I0_test),len(I1_test))\n",
    "    I0_test_rw=I0_test[range(len_rw)]\n",
    "    I1_test_rw=I1_test[range(len_rw)]\n",
    "    Dtest_rw=Dtest.loc[I0_test_rw.tolist()+I1_test_rw.tolist()]\n",
    "    \n",
    "    # update labels, based on thresholds\n",
    "    #all_thresh = np.linspace(0.2, 0.8, 10)\n",
    "    #for thresh in all_thresh:  \n",
    "    thresh=[]\n",
    "    for m in methods:\n",
    "        model_perf=fair_metric(Dtest,I0_test,I1_test,m,thresh,base_rate)\n",
    "        model_perf_rw=fair_metric(Dtest_rw,I0_test_rw,I1_test_rw,m,thresh,base_rate)\n",
    "        metric=metric.append({'IND':model_perf[0],'SP':model_perf[1],'SF':model_perf[2],'INA':model_perf[3],\n",
    "                              'INDrw':model_perf_rw[0],'SPrw':model_perf_rw[1],'SFrw':model_perf_rw[2],'INArw':model_perf_rw[3],\n",
    "                              'type':m,'thresh':0.5,'trial':ignore},ignore_index=True)\n",
    "\n",
    "metric.to_csv('data/COMPAS4_metric_AIF360.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:03<00:00, 12.80s/it]\n"
     ]
    }
   ],
   "source": [
    "## ====================\n",
    "trials=5\n",
    "\n",
    "methods=['COMPAS',\"fnr\", \"fpr\", \"weighted\",'RejectOption','EqOddsPostprocessing'] #\"fnr\", \"fpr\", \"weighted\"\n",
    "\n",
    "metric=pd.DataFrame(columns=performace+['type','thresh','trial'])\n",
    "\n",
    "for ignore in tqdm(range(trials)):\n",
    "    \n",
    "    Itrain,Itest=split_index(dataframe.index.tolist())\n",
    "    Dtest=dataframe.loc[Itest]\n",
    "    I0_test=Dtest[Dtest['race']=='Caucasian'].index\n",
    "    I1_test=Dtest[Dtest['race']!='Caucasian'].index\n",
    "\n",
    "    # update scores:\n",
    "    Dtest['COMPAS']=Dtest['decile_score']/10\n",
    "    for m in methods[1:]:\n",
    "        if m not in ['subgroup-fair',\"instant-fair\"]:\n",
    "            pp=aif_postprocess(dataframe,Itrain,Itest,m).flatten()\n",
    "        else:\n",
    "            pp=new_postprocess(dataframe,Itrain,Itest,m)\n",
    "        i=0\n",
    "        for j in Itest:\n",
    "            Dtest.loc[j,m]=pp[i]\n",
    "            i+=1\n",
    "            \n",
    "    len_rw=min(len(I0_test),len(I1_test))\n",
    "    I0_test_rw=I0_test[range(len_rw)]\n",
    "    I1_test_rw=I1_test[range(len_rw)]\n",
    "    Dtest_rw=Dtest.loc[I0_test_rw.tolist()+I1_test_rw.tolist()]\n",
    "\n",
    "    # update labels, based on thresholds\n",
    "    all_thresh = np.linspace(20, 80, 10)\n",
    "    for thresh in all_thresh:  \n",
    "        for m in methods:\n",
    "            model_perf=fair_metric(Dtest,I0_test,I1_test,m,thresh,[]) \n",
    "            model_perf_rw=fair_metric(Dtest_rw,I0_test_rw,I1_test_rw,m,thresh,[])\n",
    "            metric=metric.append({'IND':model_perf[0],'SP':model_perf[1],'SF':model_perf[2],'INA':model_perf[3],\n",
    "                                  'INDrw':model_perf_rw[0],'SPrw':model_perf_rw[1],'SFrw':model_perf_rw[2],'INArw':model_perf_rw[3],\n",
    "                                  'type':m,'thresh':round(thresh),'trial':ignore},ignore_index=True)\n",
    "\n",
    "metric.to_csv('data/AIF3_metric.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:08<00:00,  5.98it/s]\n"
     ]
    }
   ],
   "source": [
    "## ===================\n",
    "trials=50\n",
    "\n",
    "methods=['COMPAS',\"weighted\"] #,\"subgroup-fair\",\"instant-fair\"\n",
    "\n",
    "metric=pd.DataFrame(columns=performace+['type','thresh','trial'])\n",
    "\n",
    "for ignore in tqdm(range(trials)):\n",
    "    \n",
    "    Itrain,Itest=split_index(dataframe.index.tolist())\n",
    "    Dtest=dataframe.loc[Itest]\n",
    "    I0_test=Dtest[Dtest['race']=='Caucasian'].index\n",
    "    I1_test=Dtest[Dtest['race']!='Caucasian'].index\n",
    "\n",
    "    # update scores:\n",
    "    Dtest['COMPAS']=Dtest['decile_score']/10\n",
    "    for m in methods[1:]:\n",
    "        if m not in ['subgroup-fair',\"instant-fair\"]:\n",
    "            pp=aif_postprocess(dataframe,Itrain,Itest,m).flatten()\n",
    "        else:\n",
    "            pp=new_postprocess(dataframe,Itrain,Itest,m)\n",
    "        i=0\n",
    "        for j in Itest:\n",
    "            Dtest.loc[j,m]=pp[i]\n",
    "            i+=1\n",
    "            \n",
    "    len_rw=min(len(I0_test),len(I1_test))\n",
    "    I0_test_rw=I0_test[range(len_rw)]\n",
    "    I1_test_rw=I1_test[range(len_rw)]\n",
    "    Dtest_rw=Dtest.loc[I0_test_rw.tolist()+I1_test_rw.tolist()]\n",
    "\n",
    "    # update labels, based on thresholds\n",
    "    all_thresh = np.linspace(20, 80, 10)\n",
    "    for thresh in all_thresh:  \n",
    "        for m in methods:\n",
    "            model_perf=fair_metric(Dtest,I0_test,I1_test,m,thresh,[]) \n",
    "            model_perf_rw=fair_metric(Dtest_rw,I0_test_rw,I1_test_rw,m,thresh,[])\n",
    "            metric=metric.append({'IND':model_perf[0],'SP':model_perf[1],'SF':model_perf[2],'INA':model_perf[3],\n",
    "                                  'INDrw':model_perf_rw[0],'SPrw':model_perf_rw[1],'SFrw':model_perf_rw[2],'INArw':model_perf_rw[3],\n",
    "                                  'type':m,'thresh':round(thresh),'trial':ignore},ignore_index=True)\n",
    "\n",
    "metric.to_csv('data/COMPAS4_metric_thresh_AIF360.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
